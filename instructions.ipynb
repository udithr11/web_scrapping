{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOt6imSblyXApEDAyqx7gi/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"Instructions:\n","\n","\n","I have done the coding using google collab.\n","The “input.xlxs” is uploaded in the root folder in collab and the “master dictionary” and “stop words” in the home folder in the collab.\n","\n","\n","Part 1:\n","The web scraping is done by using beautiful soup library.\n","Two cases have implemented to get the content of the articles.\n","Some article link is giving 404 error , so this case is includes using except functionality.\n","The extracted text is saved using the url_id provided in the input file\n"," \n","Part2:\n","From each extracted text file the stop words from the given stop words folder is excluded and saved in the same location using the same name.\n","Then each file is opened and parallely the index number of the file in the dataframe is extracted. \n","The different features of the text file is calculated and is saved in the dataframe corresponding to the index number.\n","The final result is stored in the input dataframe and is converted to the csv format and saved as “output.csv”.\n","Few rows of the output.csv will be NaN values as the corresponding links are showing 404 error.\"\"\""],"metadata":{"id":"zRUGAvqTwBvm"},"execution_count":null,"outputs":[]}]}