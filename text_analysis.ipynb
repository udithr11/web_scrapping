{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNR6AeKEJeampf26vn0NEjY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9B3yrBtNVb0v","executionInfo":{"status":"ok","timestamp":1682604517466,"user_tz":-330,"elapsed":97145,"user":{"displayName":"Udith R","userId":"13404142171011772478"}},"outputId":"a13ba4ea-2356-48dc-ff5e-0e06f33f33e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#function to count the syllables in a word\n","def count_syllables(word):\n","\n","    vowels = \"aeiouy\"\n","    num_vowels = 0\n","    last_was_vowel = False\n","    for letter in word:\n","        if letter in vowels:\n","            # don't count consecutive vowels\n","            if not last_was_vowel:\n","                num_vowels += 1\n","            last_was_vowel = True\n","        else:\n","            last_was_vowel = False\n","\n","    # handle some common exceptions\n","    if word[-2:] == \"es\" or word[-2:] == \"ed\":\n","        num_vowels -= 1\n","    elif word[-1:] == \"e\":\n","        num_vowels -= 1\n","\n","    return max(1, num_vowels)\n","\n","\n","\n","#inputing the required libraries\n","\n","from flask import Flask , render_template , request , jsonify\n","from bs4 import BeautifulSoup as bs \n","import pandas as pd\n","from urllib.request import urlopen as urReq\n","import requests\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","import os\n","\n","\n","#reading the input file\n","input=pd.read_excel('/Input.xlsx')\n","\n","limit=input.shape[0]\n","\n","#extracting the content by iterating through each link in the input file\n","for i in range(1,limit):\n","  try:\n","      url=input['URL'][i]\n","      url_id=input[\"URL_ID\"][i]\n","\n","      #getting the title of the page\n","      response = requests.get(url)\n","      soup = bs(response.content, 'html.parser')\n","      title = soup.title.string\n","      q=str(title)\n","      q=q.split(\"|\")\n","      title=q[0]\n","\n","      #getting the content of the page\n","      response_website = urReq(url)\n","      data = response_website.read()\n","      beautifyed_html  = bs(data,\"html.parser\")\n","\n","      soup = bs(data, 'html.parser')\n","\n","      div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n","      if div_tag:\n","          p_tags = div_tag.find_all('p')\n","          article_text=\"\"\n","          for tag in p_tags:\n","              article_text+=(tag.text.strip())\n","          final_article=title+article_text\n","      else:\n","        #some contents of the page is in a combined class\n","        div_tag = soup.find('div', class_='td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n","        if div_tag:\n","            p_tags = div_tag.find_all('p')\n","            article_text=\"\"\n","            for tag in p_tags:\n","                article_text+=(tag.text.strip())\n","            final_article=title+article_text\n","        final_article\n","        \n","\n","      #writing to the file\n","      with open(f\"{url_id}.txt\", \"w\") as f:\n","          f.write(final_article)\n","  except:\n","    #for the links which shows 404 error\n","    pass\n","\n","\n","\n","\n","\n","\n","\n","#adding only those words in the extracted file if they are not found in the Stop Words Lists. \n","\n","\n","stop_words = set()\n","\n","for filename in os.listdir('/home/stopwords'):\n","    if filename.endswith('.txt'):\n","        with open(os.path.join('/home/stopwords', filename), 'r', encoding='ISO-8859-1') as f:\n","            stop_words.update(set(f.read().splitlines()))  # Add the stop words from the file to the set\n","\n","stop_words=list(stop_words)\n","for i in range(len(stop_words)):\n","  stop_words[i]=stop_words[i].upper() #final stop word list\n","\n","\n","for filename in os.listdir('/content'):\n","    if filename.endswith('.txt'):\n","        with open(os.path.join('/content', filename), 'r') as f:\n","            text = f.read()\n","            words = word_tokenize(text)  # Tokenize the text into words\n","            filtered_text = [word for word in words if word.upper() not in stop_words]\n","            filtered_text = \" \".join(filtered_text)\n","\n","\n","\n","\n","        with open(os.path.join('/content', filename), 'w') as f:\n","            f.write(filtered_text) # rewriting the file with no stop words to the same location \n","\n","\n","\n","positive_count = 0\n","negative_count = 0\n","\n","positive_words = set()\n","with open(\"/home/master_dictionary/positive-words.txt\", \"r\", encoding='ISO-8859-1') as f:\n","    for line in f:\n","        positive_words.add(line.strip())\n","negative_words = set()\n","with open(\"/home/master_dictionary/negative-words.txt\", \"r\", encoding='ISO-8859-1') as f:\n","    for line in f:\n","        negative_words.add(line.strip())\n","\n","\n","for filename in os.listdir('/content'):\n","    if filename.endswith('.txt'):  \n","        with open(os.path.join('/content', filename), 'r') as f:\n","          text = f.read()\n","          index=filename.split(\".\")\n","          #finding the index of the each file name based on url_id so we edit the dataframe\n","          index=(int(index[0]))\n","          p=(input[input.URL_ID==index].index)\n","          index=int(p[0]) \n","\n","          #positive and negative\n","          words = text.split()\n","          for word in words:\n","              if word in positive_words:\n","                  positive_count += 1\n","              elif word in negative_words:\n","                  negative_count += 1\n","\n","          input.loc[index, 'positive_score'] = positive_count   #creating a new column in the input dataframe and inputting values \n","          input.loc[index, 'negative_score'] = negative_count\n","\n","#polarity subjectivity\n","          Polarity_score = (positive_count - negative_count)/ ((positive_count + negative_count) + 0.000001)\n","          input.loc[index, 'Polarity score'] = Polarity_score\n","\n","          total_words_after_cleaning=len(words)\n","          Subjectivity_Score = (positive_count+ negative_count)/ ((total_words_after_cleaning) + 0.000001)\n","          input.loc[index, 'Subjectivity score'] = Subjectivity_Score\n","\n","\n","#avg sentence length\n","\n","          sentences = re.split(r'[.!?]+', text) #separating the text into sentences\n","          total_sentences = len(sentences)\n","          total_words_after_cleaning=len(words)\n","          Average_sentence_length=total_words_after_cleaning/total_sentences\n","          input.loc[index, 'Average_sentence_length'] = Average_sentence_length\n","\n","#PERCENTAGE OF COMPLEX WORDS\n","          complex_words = re.findall(r'\\b\\w{3,}[^aeiou\\s\\d]*[aeiouy][^aeiou\\s]*\\b', text) #looking for atlest two vowels in a word \n","          complex_no=len(complex_words)\n","          input.loc[index, 'complex word count'] = complex_no\n","\n","#fog index\n","          percentage_complex = (complex_no/total_words_after_cleaning)\n","          Fog_Index = 0.4 * (Average_sentence_length + percentage_complex)\n","          input.loc[index, 'Fog_index'] = Fog_Index\n","\n","\n","#avg no.of words per sentences\n","          sentences = re.split(r'[.!?]+', text)\n","          total_sentences = len(sentences)\n","          words_per_sentence = [len(sentence.split()) for sentence in sentences]\n","          avg_words_per_sentence = sum(words_per_sentence) / len(words_per_sentence)\n","          input.loc[index, 'avg_words_per_sentence'] = avg_words_per_sentence\n","    \n","\n","#complex word count\n","          input.loc[index, 'percentage of complex words'] = percentage_complex\n","\n","\n","#WORD COUNT\n","          sentences=nltk.sent_tokenize(text)\n","          corpus=\"\"\n","          for i in range(len(sentences)):\n","            review=re.sub('[^a-zA-Z]',' ',sentences[i]) # excluding all special characters\n","            review=review.lower()\n","            review=review.split()\n","            review=[word for word in review if not word in set(stopwords.words('english'))] #looking for words present in the stopwords of english\n","            review=' '.join(review)\n","            corpus+=review\n","          word_count=len(corpus)\n","          input.loc[index, 'word count'] = word_count\n","\n","\n","#SYLLABLE PER WORD\n","          words = text.split()\n","          syllables = [count_syllables(word.lower()) for word in words] #calling function to find the syllable count\n","          total_syllables = sum(syllables)\n","          input.loc[index, 'total_syllables'] = total_syllables\n","\n","\n","\n","\n","#PERSONAL PRONOUNS\n","          words = text.split()\n","          #finding personal pronouns in the file (which do not have the stop words from the stop words of the given Stopwords folder)\n","          pronoun_count = re.compile(r'\\b(I|we|ours|my|mine|(?-i:us))\\b', re.I)\n","          pronouns = pronoun_count.findall(text)\n","          Personal_pronouns=len(pronouns)\n","          input.loc[index, 'Personal_pronouns'] = Personal_pronouns\n","\n","#AVG WORD LENGTH\n","          words = text.split()\n","          total_chars = sum(len(word) for word in words)\n","          total_words = len(words)\n","          average_word_length = total_chars / total_words\n","          input.loc[index, 'average_word_length'] = average_word_length\n","\n","\n","\n","\n","\n","#creating output file in csv format\n","\n","input.to_csv('output.csv', index=False)\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"wdjY33Q44wlI","executionInfo":{"status":"ok","timestamp":1682604338183,"user_tz":-330,"elapsed":646,"user":{"displayName":"Udith R","userId":"13404142171011772478"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DbK1UoGxBmih"},"execution_count":null,"outputs":[]}]}